{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import hydra\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.distributions import MultivariateNormal, Independent, Normal, Categorical\n",
    "\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import KFModels as KFM\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import matplotlib as plt\n",
    "import nonamortised_models as models\n",
    "\n",
    "import Data_Generation as DG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize hydra and savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Seed: None\n",
      "Save Models: True\n",
      "Save Data: True\n",
      "Data Config: path_to_data: null\n",
      "dim: 10\n",
      "F_min_eigval: 0.5\n",
      "F_max_eigval: 1\n",
      "G_min_eigval: 0.5\n",
      "G_max_eigval: 1\n",
      "diagFG: true\n",
      "U_std: 0.1\n",
      "V_std: 0.25\n",
      "num_data: 50000\n",
      "matrices_to_learn: FG\n",
      "\n",
      "Theta Training Config: func_type: kernel\n",
      "theta_lr: 0.03\n",
      "theta_lr_decay_type: robbins-monro\n",
      "num_steps_theta_lr_oom_drop: 5000\n",
      "robbins_monro_theta_lr_decay_rate: 0.6\n",
      "robbins_monro_theta_lr_decay_bias: 10\n",
      "theta_minibatch_size: 100\n",
      "theta_updates_start_T: 5\n",
      "matrices_to_learn: FG\n",
      "window_size: 1\n",
      "num_times_save_data: 100\n",
      "KRR_lambda: 0.0001\n",
      "KRR_sigma: 2\n",
      "KRR_train_sigma: true\n",
      "KRR_train_lam: false\n",
      "KRR_init_sigma_median: true\n",
      "approx_decay: 1.0\n",
      "approx_with_filter: true\n",
      "kernel_batch_size: 100\n",
      "train_kernel_iters: 1\n",
      "train_kernel_minibatch_size: 100\n",
      "train_kernel_dataset_size: 100\n",
      "train_kernel_lr: 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/sqc7r28j1qggd73l6jk91xnr0000gn/T/ipykernel_49966/2254655681.py:2: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"configurations\")\n",
      "/Users/gianluca.palmari/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'conf1.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/gianluca.palmari/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'theta_training/kernel1.yaml': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/Users/gianluca.palmari/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'phi_training/V_function1.yaml': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/Users/gianluca.palmari/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'data/linear_gaussian_for_config1': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize OmegaConf\n",
    "initialize(config_path=\"configurations\")\n",
    "\n",
    "# Compose the configuration\n",
    "cfg = compose(config_name='conf1.yaml')\n",
    "\n",
    "# Access nested configurations using keys\n",
    "data_cfg = cfg['data']\n",
    "theta_training_cfg = cfg['theta_training']\n",
    "phi_training_cfg = cfg['phi_training']\n",
    "\n",
    "# Now you can access specific attributes within these nested configurations\n",
    "device = cfg.device\n",
    "seed = cfg.seed\n",
    "save_models = cfg.save_models\n",
    "save_data = cfg.save_data\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Save Models:\", save_models)\n",
    "print(\"Save Data:\", save_data)\n",
    "\n",
    "# Print the nested configurations\n",
    "print(\"Data Config:\", OmegaConf.to_yaml(data_cfg))\n",
    "print(\"Theta Training Config:\", OmegaConf.to_yaml(theta_training_cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'phi_training', 'theta_training', 'name', 'device', 'seed', 'save_models', 'save_data'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 3553697\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_MODE = True\n",
    "\n",
    "# Generate and print seed\n",
    "seed = np.random.randint(0, 9999999) if cfg.seed is None else cfg.seed\n",
    "print(\"seed\", seed)\n",
    "\n",
    "# Seed numpy and torch\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Create saved_models folder if required\n",
    "saved_models_folder_name = 'saved_models'\n",
    "if cfg.save_models and not NOTEBOOK_MODE:\n",
    "    if not os.path.exists(saved_models_folder_name):  \n",
    "        os.mkdir(saved_models_folder_name)  \n",
    "elif cfg.save_models and NOTEBOOK_MODE:\n",
    "    if not os.path.exists(saved_models_folder_name): \n",
    "        os.mkdir(saved_models_folder_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create saved_data folder if required\n",
    "saved_data_folder_name = 'saved_data'\n",
    "if cfg.save_data and not NOTEBOOK_MODE:\n",
    "    if not os.path.exists(saved_data_folder_name):  \n",
    "        os.mkdir(saved_data_folder_name)  \n",
    "elif cfg.save_data and NOTEBOOK_MODE:\n",
    "    if not os.path.exists(saved_data_folder_name): \n",
    "        os.mkdir(saved_data_folder_name)  \n",
    "\n",
    "# Function to save numpy array\n",
    "def save_np(name, x):\n",
    "    if NOTEBOOK_MODE:\n",
    "        np.save(f'saved_data/{name}', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Utils:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeStore(nn.Module):\n",
    "    \"\"\"\n",
    "        Acts like a ParameterList/ModuleList that just keeps getting longer but\n",
    "        under the hood it only stores the most recent N values.\n",
    "        Does not support removing items from the list.\n",
    "    \"\"\"\n",
    "    def __init__(self, first_val, N, type):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == \"ParameterList\":\n",
    "            self.list = nn.ParameterList([first_val] if first_val else [])\n",
    "        elif type == \"ModuleList\":\n",
    "            self.list = nn.ModuleList([first_val] if first_val else [])\n",
    "        else:\n",
    "            raise ValueError(\"TimeStore unknown type: \" + type)\n",
    "        self.len = 1 if first_val else 0\n",
    "        self.N = N\n",
    "\n",
    "    def append(self, val):\n",
    "        self.list.append(val)\n",
    "        self.len += 1\n",
    "        if len(self.list) > self.N:\n",
    "            self.list = self.list[-self.N:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        adjusted_idx = index - self.len + len(self.list)\n",
    "\n",
    "        if index < 0:\n",
    "            raise ValueError(\"TimeStore does not support negative indexing\")\n",
    "        if index >= self.len:\n",
    "            raise ValueError(\"TimeStore index {} is too large\".format(index))\n",
    "        if adjusted_idx < 0:\n",
    "            raise ValueError(\"TimeStore attempting to access deleted entry\")\n",
    "\n",
    "        return self.list[adjusted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_posterior(y, prior_mean, prior_cov, G, V, G_fn=None):\n",
    "    # Compute posterior mean, cov of N(x; prior_mean, prior_cov) N(y; G @ x, V)\n",
    "    # y: (*, ydim), prior_mean: (*, xdim), prior_cov: (*, xdim, xdim),\n",
    "    # G: (*, ydim, xdim), V: (*, ydim, ydim)\n",
    "    K = prior_cov @ G.transpose(-2, -1) @ torch.linalg.inv(G @ prior_cov @ G.transpose(-2, -1) + V)\n",
    "    I_KG = torch.eye(prior_cov.shape[-1], device=prior_cov.device) - K @ G\n",
    "    post_cov = I_KG @ prior_cov\n",
    "    if G_fn is None:\n",
    "        post_mean = (prior_mean.unsqueeze(-2) @ I_KG.transpose(-2, -1) +\n",
    "                     y.unsqueeze(-2) @ K.transpose(-2, -1)).squeeze(-2)\n",
    "    else:\n",
    "        post_mean = prior_mean + ((y - G_fn(prior_mean)).unsqueeze(-2) @ K.transpose(-2, -1)).squeeze(-2)\n",
    "    return post_mean, post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov(x, y=None, w=None):\n",
    "    assert len(x.shape) == 2\n",
    "    if w is None:\n",
    "        w = 1 / x.shape[0] * torch.ones((x.shape[0], 1)).to(x.device)\n",
    "    else:\n",
    "        w = w.view(x.shape[0], 1)\n",
    "    x_centred = x - (x * w).sum(0)\n",
    "    if y is None:\n",
    "        y_centred = x_centred\n",
    "    else:\n",
    "        y_centred = y - (y * w).sum(0)\n",
    "    cov = (w * x_centred).t() @ y_centred / (1 - (w**2).sum())\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils as utils\n",
    "def estimate_joint_kl(model, num_samples, kalman_mean_T, kalman_cov_T,\n",
    "                        kalman_mean_Tm1, kalman_cov_Tm1, true_F, true_U):\n",
    "        \"\"\"\n",
    "            Estimates KL (q(x_{t-1}, x_t | y_{1:t}) || p(x_{t-1}, x_t | y_{1:t}))\n",
    "            using num_samples\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x_samples, all_q_stats = model.sample_joint_q_t(num_samples, 1)\n",
    "            x_Tm1, x_T = x_samples\n",
    "            log_q_t = model.compute_log_q_t(x_T, *all_q_stats[1])\n",
    "            log_q_t_1 = model.compute_log_q_t(x_Tm1, *all_q_stats[0])\n",
    "\n",
    "            log_p_x = utils.back_1_joint_smoothing_prob(x_T, x_Tm1,\n",
    "                                                        kalman_mean_T, kalman_cov_T,\n",
    "                                                        kalman_mean_Tm1, kalman_cov_Tm1,\n",
    "                                                        true_F, true_U)\n",
    "\n",
    "            return torch.mean(log_q_t + log_q_t_1 - log_p_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Data\\text{ }Generation:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True F:  [[0.979997   0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.76476305 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.53951532 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.90432027 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.86441557 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.88183115\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.92812136 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.57421853 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.69921678 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.89462601]]\n",
      "True G:  [[0.59523581 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.55630805 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.9886417  0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.54285754 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.59029988 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.88803456\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.89147713 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66114741 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99168366 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.95186027]]\n"
     ]
    }
   ],
   "source": [
    "DIM = cfg.data.dim\n",
    "\n",
    "if not cfg.data.diagFG:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if cfg.data.path_to_data is None:\n",
    "    F, G, U, V = DG.construct_HMM_matrices(dim=DIM,\n",
    "                                        F_eigvals=np.random.uniform(\n",
    "                                            cfg.data.F_min_eigval,\n",
    "                                            cfg.data.F_max_eigval, (DIM)),\n",
    "                                        G_eigvals=np.random.uniform(\n",
    "                                            cfg.data.G_min_eigval,\n",
    "                                            cfg.data.G_max_eigval, (DIM)),\n",
    "                                        U_std=cfg.data.U_std,\n",
    "                                        V_std=cfg.data.V_std,\n",
    "                                        diag=cfg.data.diagFG)\n",
    "\n",
    "    data_gen = DG.GaussianHMM(xdim=DIM, ydim=DIM, F=F, G=G, U=U, V=V)\n",
    "    x_np, y_np = data_gen.generate_data(cfg.data.num_data)\n",
    "\n",
    "    save_np('datapoints.npy', np.stack((x_np, y_np)))\n",
    "    save_np('F.npy', F)\n",
    "    save_np('G.npy', G)\n",
    "    save_np('U.npy', U)\n",
    "    save_np('V.npy', V)\n",
    "else:\n",
    "    path_to_data = hydra.utils.to_absolute_path(cfg.data.path_to_data) + '/'\n",
    "    F, G, U, V = np.load(path_to_data + 'F.npy'), \\\n",
    "                np.load(path_to_data + 'G.npy'), \\\n",
    "                np.load(path_to_data + 'U.npy'), \\\n",
    "                np.load(path_to_data + 'V.npy')\n",
    "    xystack = np.load(path_to_data + 'datapoints.npy')\n",
    "    x_np = xystack[0, :, :]\n",
    "    y_np = xystack[1, :, :]\n",
    "\n",
    "print(\"True F: \", F)\n",
    "print(\"True G: \", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Kalman-Filter:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_kalman_phi_update(model, T, G, F, U, V, y_T):\n",
    "    '''\n",
    "    xtp1 = Fxtp + w, w sim N(0,U)\n",
    "    yt = Gxt + v, w sim N(0,V)\n",
    "\n",
    "    Pp: cov of the nonadapted error\n",
    "    K: Kalman gain\n",
    "    z: the innovation\n",
    "    new_mean: adapted x\n",
    "    new_cov: new error covariance\n",
    "    cond_cov: \n",
    "    '''\n",
    "    \n",
    "    prev_mean = model.q_t_mean_list[T-1].reshape(model.xdim, 1)\n",
    "    prev_cov = torch.diag(torch.exp(2*model.q_t_log_std_list[T-1]))\n",
    "    y_T = y_T.reshape(model.ydim, 1)\n",
    "\n",
    "    xp = F @ prev_mean\n",
    "    Pp = F @ prev_cov @ F.T + U\n",
    "    S = G @ Pp @ G.T + V\n",
    "    K = Pp @ G.T @ torch.inverse(S)\n",
    "    z = y_T - G @ xp\n",
    "\n",
    "    new_mean = xp + K @ z\n",
    "    new_cov = (torch.eye(model.xdim).to(device) - K @ G) @ Pp\n",
    "\n",
    "    cond_cov = torch.inverse(torch.inverse(prev_cov) + \\\n",
    "        F.T @ torch.inverse(U) @ F) \n",
    "    cond_weight = cond_cov @ F.T @ torch.inverse(U)\n",
    "    cond_bias = cond_cov @ torch.inverse(prev_cov) @ prev_mean\n",
    "\n",
    "    model.q_t_mean_list[T].data = new_mean.reshape(model.xdim)\n",
    "    model.q_t_log_std_list[T].data = 0.5 * torch.log(torch.diag(new_cov))\n",
    "    model.cond_q_t_mean_net_list[T].weight.data = cond_weight\n",
    "    model.cond_q_t_mean_net_list[T].bias.data = cond_bias.reshape(model.xdim)\n",
    "    model.cond_q_t_log_std_list[T].data = 0.5 * torch.log(torch.diag(cond_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_xs = np.zeros((y_np.shape[0], DIM))\n",
    "kalman_Ps = np.zeros((y_np.shape[0], DIM, DIM))\n",
    "\n",
    "# For t=0\n",
    "kalman_Ps[0, :, :] = np.linalg.inv(np.eye(DIM) + G.T @ np.linalg.inv(V) @ G)\n",
    "kalman_xs[0, :] = kalman_Ps[0, :, :] @ G.T @ np.linalg.inv(V) @ y_np[0, :]\n",
    "\n",
    "kalman_filter = KFM.KalmanFilter(x_0=kalman_xs[0, :], P_0=kalman_Ps[0, :, :], F=F, G=G, U=U,\n",
    "                                    V=V)\n",
    "\n",
    "for t in range(1, y_np.shape[0]):\n",
    "    kalman_filter.update(y_np[t, :])\n",
    "    kalman_xs[t, :] = kalman_filter.x\n",
    "    kalman_Ps[t, :, :] = kalman_filter.P\n",
    "kalman_xs_pyt = torch.from_numpy(kalman_xs).float()\n",
    "kalman_Ps_pyt = torch.from_numpy(kalman_Ps).float()\n",
    "\n",
    "F_init, G_init, _, _ = DG.construct_HMM_matrices(dim=DIM,\n",
    "                                        F_eigvals=np.random.uniform(\n",
    "                                            cfg.data.F_min_eigval,\n",
    "                                            cfg.data.F_max_eigval, (DIM)),\n",
    "                                        G_eigvals=np.random.uniform(\n",
    "                                            cfg.data.G_min_eigval,\n",
    "                                            cfg.data.G_max_eigval, (DIM)),\n",
    "                                        U_std=cfg.data.U_std,\n",
    "                                        V_std=cfg.data.V_std,\n",
    "                                        diag=cfg.data.diagFG)\n",
    "\n",
    "y = torch.from_numpy(y_np).float().to(device)\n",
    "\n",
    "F = torch.from_numpy(F).float().to(device)\n",
    "G = torch.from_numpy(G).float().to(device)\n",
    "\n",
    "F_init = torch.from_numpy(F_init).float().to(device)\n",
    "G_init = torch.from_numpy(G_init).float().to(device)\n",
    "\n",
    "U = torch.from_numpy(U).float().to(device)\n",
    "V = torch.from_numpy(V).float().to(device)\n",
    "\n",
    "mean_0 = torch.zeros(DIM).to(device)\n",
    "std_0 = torch.sqrt(torch.diag(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9381, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.9515, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8241, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7636, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7628, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8681, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7861, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7239,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7357]])\n",
      "tensor([[0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.7648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5395, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9043, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8644, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8818, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9281, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5742, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6992,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8946]])\n"
     ]
    }
   ],
   "source": [
    "print(F_init)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(kalman_xs_pyt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(kalman_Ps_pyt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Model\\text{ }parametrization:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the observalbe's transiton map\n",
    "class F_Module(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.register_parameter('weight',\n",
    "                nn.Parameter(torch.zeros(DIM)))\n",
    "            self.F_mean_fn = lambda x, t: self.weight * x\n",
    "            self.F_cov_fn = lambda x, t: U\n",
    "            self.F_cov = U\n",
    "\n",
    "        def forward(self, x, t=None):\n",
    "            return Independent(Normal(self.F_mean_fn(x, t),\n",
    "                torch.sqrt(torch.diag(U))), 1)\n",
    "\n",
    "# we define the latent's transiton map\n",
    "class G_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_parameter('weight',\n",
    "            nn.Parameter(torch.zeros(DIM)))\n",
    "        self.G_mean_fn = lambda x, t: self.weight * x\n",
    "        self.G_cov = V\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        return Independent(Normal(self.G_mean_fn(x, t),\n",
    "            torch.sqrt(torch.diag(V))), 1)\n",
    "            \n",
    "# we define the initialization class\n",
    "class p_0_dist_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mean_0 = mean_0\n",
    "        self.cov_0 = torch.eye(DIM, device=device) * std_0 ** 2\n",
    "\n",
    "    def forward(self):\n",
    "        return Independent(Normal(mean_0, std_0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the two linking modules and the initial prior on the latent variable.\n",
    "F_fn = F_Module().to(device)\n",
    "G_fn = G_Module().to(device)\n",
    "p_0_dist = p_0_dist_module().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Parameters:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G theta dim 10\n",
      "F theta dim 10\n",
      "Theta dim 20\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters to be learned\n",
    "G_theta_dim = sum([p.numel() for p in G_fn.parameters()])\n",
    "F_theta_dim = sum([p.numel() for p in F_fn.parameters()])\n",
    "print(\"G theta dim\", G_theta_dim)\n",
    "print(\"F theta dim\", F_theta_dim)\n",
    "if cfg.data.matrices_to_learn == 'F':\n",
    "    theta_dim = F_theta_dim\n",
    "elif cfg.data.matrices_to_learn == 'G':\n",
    "    theta_dim = G_theta_dim\n",
    "elif cfg.data.matrices_to_learn == 'FG':\n",
    "    theta_dim = F_theta_dim + G_theta_dim\n",
    "else:\n",
    "    raise ValueError(cfg.data.matrices_to_learn)\n",
    "print(\"Theta dim\", theta_dim)\n",
    "\n",
    "# get the model parameters\n",
    "def get_model_parameters():\n",
    "        if cfg.data.matrices_to_learn == 'F':\n",
    "            return F_fn.parameters()\n",
    "        elif cfg.data.matrices_to_learn == 'G':\n",
    "            return G_fn.parameters()\n",
    "        elif cfg.data.matrices_to_learn == 'FG':\n",
    "            return [*F_fn.parameters(), *G_fn.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Variational\\text{ }Model\\text{ }Specifications-(\\phi):}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using phi model Vx_t\n"
     ]
    }
   ],
   "source": [
    "def cond_q_mean_net_constructor():\n",
    "        return torch.nn.Linear(DIM, DIM).to(device)\n",
    "\n",
    "if cfg.phi_training.func_type == 'Vx_t':\n",
    "    print(\"Using phi model Vx_t\")\n",
    "\n",
    "    sigma = cfg.phi_training.KRR_sigma\n",
    "    lam = cfg.phi_training.KRR_lambda\n",
    "    train_sigma = cfg.phi_training.KRR_train_sigma\n",
    "    train_lam = cfg.phi_training.KRR_train_lam\n",
    "\n",
    "    def KRR_constructor():\n",
    "        return models.KernelRidgeRegressor(models.MaternKernel(\n",
    "            sigma=sigma, lam=lam, train_sigma=train_sigma, train_lam=train_lam)).to(device)\n",
    "\n",
    "    phi_model = models.Vx_t_phi_t_Model(\n",
    "        device, DIM, DIM, torch.randn(DIM, device=device),\n",
    "        torch.zeros(DIM, device=device), cond_q_mean_net_constructor,\n",
    "        torch.zeros(DIM, device=device), F_fn, G_fn, p_0_dist,\n",
    "        cfg.phi_training.phi_t_init_method,\n",
    "        cfg.phi_training.window_size,\n",
    "        KRR_constructor, cfg.phi_training.KRR_init_sigma_median,\n",
    "        cfg.phi_training.approx_decay,\n",
    "        cfg.phi_training.approx_with_filter,\n",
    "        max(cfg.phi_training.window_size, cfg.theta_training.window_size)+1\n",
    "    )\n",
    "\n",
    "elif cfg.phi_training.func_type == 'analytic':\n",
    "    print(\"Using analytic phi updates\")\n",
    "    phi_model = models.NonAmortizedModelBase(\n",
    "        device, DIM, DIM, torch.zeros(DIM, device=device),\n",
    "        torch.zeros(DIM, device=device), cond_q_mean_net_constructor,\n",
    "        torch.zeros(DIM, device=device), F_fn, G_fn, p_0_dist,\n",
    "        'last', 1, cfg.theta_training.window_size + 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Real\\text{ }Model\\text{ }Specifications-(\\theta)}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning theta grads with kernel\n"
     ]
    }
   ],
   "source": [
    "matrices_to_learn = cfg.theta_training.matrices_to_learn\n",
    "\n",
    "def add_theta_grads_to_params(grads):\n",
    "    if matrices_to_learn == 'G':\n",
    "        phi_model.G_fn.weight.grad += grads\n",
    "\n",
    "    elif matrices_to_learn == 'F':\n",
    "        phi_model.F_fn.weight.grad += grads\n",
    "\n",
    "    elif matrices_to_learn == 'FG':\n",
    "        phi_model.F_fn.weight.grad += grads[:int(theta_dim/2)]\n",
    "        phi_model.G_fn.weight.grad += grads[int(theta_dim/2):]\n",
    "\n",
    "if cfg.theta_training.func_type == 'neural_net':\n",
    "    print(\"Learning theta grad with neural nets\")\n",
    "    h = cfg.theta_training.net_hidden_dim\n",
    "\n",
    "    def theta_func_constructor():\n",
    "        nnlist = [nn.Linear(DIM, h), nn.ReLU()]\n",
    "        for i in range(cfg.theta_training.net_num_hidden_layers-1):\n",
    "            nnlist += [nn.Linear(h, h), nn.ReLU()]\n",
    "        nnlist += [nn.Linear(h, theta_dim)]\n",
    "\n",
    "        return models.NNFuncEstimator(\n",
    "            nn.Sequential(*nnlist), DIM, theta_dim\n",
    "        ).to(device)\n",
    "elif cfg.theta_training.func_type == 'kernel':\n",
    "    print(\"Learning theta grads with kernel\")\n",
    "    def theta_func_constructor():\n",
    "        krr = models.KernelRidgeRegressor(\n",
    "            models.MaternKernel(\n",
    "                sigma=cfg.theta_training.KRR_sigma,\n",
    "                lam=cfg.theta_training.KRR_lambda,\n",
    "                train_sigma=cfg.theta_training.KRR_train_sigma,\n",
    "                train_lam=cfg.theta_training.KRR_train_lam\n",
    "            )\n",
    "        )\n",
    "        class KRRWrapper(nn.Module):\n",
    "            def __init__(self, krr):\n",
    "                super().__init__()\n",
    "                self.krr = krr\n",
    "\n",
    "            def fit(self, x_fit, *fs):\n",
    "                self.krr.fit(x_fit, *fs)\n",
    "            def forward(self, x):\n",
    "                return self.krr.forward(x)[0]\n",
    "            def update_K(self):\n",
    "                self.krr.update_K()\n",
    "            def train(self, mode=True):\n",
    "                return self.krr.train(mode)\n",
    "\n",
    "        return KRRWrapper(krr).to(device)\n",
    "elif cfg.theta_training.func_type == 'JELBO':\n",
    "    print(\"Learning theta grads with JELBO\")\n",
    "    def theta_func_constructor():\n",
    "        class ZeroModule(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, x):\n",
    "                return torch.zeros([x.shape[0], theta_dim]).to(x.device)\n",
    "        return ZeroModule().to(device)\n",
    "elif cfg.theta_training.func_type == 'VJF':\n",
    "    print(\"Learning theta grads with VJF\")\n",
    "    def theta_func_constructor():\n",
    "        class ZeroModule(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, x):\n",
    "                return torch.zeros([x.shape[0], theta_dim]).to(x.device)\n",
    "        return ZeroModule().to(device)\n",
    "elif cfg.theta_training.func_type == 'analytic_S':\n",
    "    print(\"Learning theta grads with analytic S\")\n",
    "    def theta_func_constructor():\n",
    "        return models.TrueSDiagFG(DIM, y, phi_model.G_fn.weight.data.clone(),\n",
    "                                    U, V, matrices_to_learn).to(device)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "if cfg.theta_training.func_type == 'VJF':\n",
    "    theta_grad = models.ThetaGradVJF(\n",
    "        device, phi_model, theta_func_constructor,\n",
    "        cfg.theta_training.window_size, theta_dim, get_model_parameters,\n",
    "        add_theta_grads_to_params\n",
    "    )\n",
    "else:\n",
    "    theta_grad = models.ThetaGradGaussian(\n",
    "        device, phi_model, theta_func_constructor,\n",
    "        cfg.theta_training.window_size, theta_dim, get_model_parameters,\n",
    "        add_theta_grads_to_params\n",
    "    )\n",
    "theta_optim = torch.optim.Adam(get_model_parameters(),\n",
    "    lr=cfg.theta_training.theta_lr)\n",
    "if cfg.theta_training.theta_lr_decay_type == 'exponential':\n",
    "    theta_decay = torch.optim.lr_scheduler.StepLR(theta_optim,\n",
    "        step_size=1, gamma=np.exp(\n",
    "            (1/cfg.theta_training.num_steps_theta_lr_oom_drop) * np.log(0.1)))\n",
    "elif cfg.theta_training.theta_lr_decay_type == 'robbins-monro':\n",
    "    lr_decay_rate = cfg.theta_training.robbins_monro_theta_lr_decay_rate\n",
    "    lr_decay_bias = cfg.theta_training.robbins_monro_theta_lr_decay_bias\n",
    "    theta_decay = torch.optim.lr_scheduler.LambdaLR(theta_optim,\n",
    "        lr_lambda=lambda epoch: lr_decay_bias / (lr_decay_bias + epoch ** lr_decay_rate))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "rmle = models.LinearRMLEDiagFG(np.zeros((DIM,1)), np.eye(DIM),\n",
    "    F_init.detach().cpu().numpy().copy() if 'F' in cfg.theta_training.matrices_to_learn else F.detach().cpu().numpy().copy(),\n",
    "    G_init.detach().cpu().numpy().copy() if 'G' in cfg.theta_training.matrices_to_learn else G.detach().cpu().numpy().copy(),\n",
    "    U.cpu().detach().numpy().copy(), V.cpu().detach().numpy().copy(),\n",
    "    cfg.theta_training.theta_lr,\n",
    "    cfg.theta_training.matrices_to_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Training\\text{ }Specifications}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_optim = torch.optim.Adam(get_model_parameters(),\n",
    "        lr=cfg.theta_training.theta_lr)\n",
    "if cfg.theta_training.theta_lr_decay_type == 'exponential':\n",
    "    theta_decay = torch.optim.lr_scheduler.StepLR(theta_optim,\n",
    "        step_size=1, gamma=np.exp(\n",
    "            (1/cfg.theta_training.num_steps_theta_lr_oom_drop) * np.log(0.1)))\n",
    "elif cfg.theta_training.theta_lr_decay_type == 'robbins-monro':\n",
    "    lr_decay_rate = cfg.theta_training.robbins_monro_theta_lr_decay_rate\n",
    "    lr_decay_bias = cfg.theta_training.robbins_monro_theta_lr_decay_bias\n",
    "    theta_decay = torch.optim.lr_scheduler.LambdaLR(theta_optim,\n",
    "        lr_lambda=lambda epoch: lr_decay_bias / (lr_decay_bias + epoch ** lr_decay_rate))\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Training:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfa6fbebccb4a1a88dc8986d1fb0685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update bandwidth to  1.9626816157537048\n",
      "Update bandwidth to  4.460809225987467\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (512, 10)) of distribution Normal(loc: torch.Size([512, 10]), scale: torch.Size([512, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mphi_training\u001b[38;5;241m.\u001b[39mphi_iters):\n\u001b[1;32m     38\u001b[0m     phi_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mphi_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_phi_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_minibatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     phi_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m     phi_decay\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/OnlineLearning/nonamortised_models.py:450\u001b[0m, in \u001b[0;36mVx_t_phi_t_Model.populate_phi_grads\u001b[0;34m(self, y, num_samples)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpopulate_phi_grads\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, num_samples):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# self.zero_grad()\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     all_r_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_and_compute_joint_r_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     r_values \u001b[38;5;241m=\u001b[39m all_r_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    453\u001b[0m     sum_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(r_values)\n",
      "File \u001b[0;32m~/Documents/OnlineLearning/nonamortised_models.py:276\u001b[0m, in \u001b[0;36mNonAmortizedModelBase.sample_and_compute_joint_r_t\u001b[0;34m(self, y, num_samples, window_size, detach_x, only_return_first_r, T)\u001b[0m\n\u001b[1;32m    274\u001b[0m x_t, q_t_stats \u001b[38;5;241m=\u001b[39m x_samples[i], all_q_stats[i]\n\u001b[1;32m    275\u001b[0m x_tm1, q_tm1_stats \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, []) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (x_samples[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], all_q_stats[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 276\u001b[0m r_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_r_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_tm1_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tm1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m r_values\u001b[38;5;241m.\u001b[39mappend(r_t)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_return_first_r:\n",
      "File \u001b[0;32m~/Documents/OnlineLearning/nonamortised_models.py:233\u001b[0m, in \u001b[0;36mNonAmortizedModelBase.compute_r_t\u001b[0;34m(self, x_t, y_t, x_tm1, t, *q_tm1_stats)\u001b[0m\n\u001b[1;32m    231\u001b[0m     r_t \u001b[38;5;241m=\u001b[39m log_p_x_t \u001b[38;5;241m+\u001b[39m log_p_y_t\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     log_p_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_p_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     log_p_x_t, log_p_y_t \u001b[38;5;241m=\u001b[39m log_p_t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_p_x_t\u001b[39m\u001b[38;5;124m\"\u001b[39m], log_p_t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_p_y_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m     log_q_x_tm1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_log_q_t(x_tm1, \u001b[38;5;241m*\u001b[39mq_tm1_stats)\n",
      "File \u001b[0;32m~/Documents/OnlineLearning/nonamortised_models.py:214\u001b[0m, in \u001b[0;36mNonAmortizedModelBase.compute_log_p_t\u001b[0;34m(self, x_t, y_t, x_tm1, t)\u001b[0m\n\u001b[1;32m    212\u001b[0m     log_p_x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_0_dist()\u001b[38;5;241m.\u001b[39mlog_prob(x_t)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     log_p_x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mF_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlog_prob(x_t)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    215\u001b[0m log_p_y_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mG_fn(x_t, t)\u001b[38;5;241m.\u001b[39mlog_prob(y_t)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_p_x_t\u001b[39m\u001b[38;5;124m\"\u001b[39m: log_p_x_t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_p_y_t\u001b[39m\u001b[38;5;124m\"\u001b[39m: log_p_y_t}\n",
      "File \u001b[0;32m~/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mF_Module.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Independent(\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mF_mean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/online_learning_env/lib/python3.9/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (512, 10)) of distribution Normal(loc: torch.Size([512, 10]), scale: torch.Size([512, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)"
     ]
    }
   ],
   "source": [
    "Gs = []\n",
    "Fs = []\n",
    "rmle_Gs = []\n",
    "rmle_Fs = []\n",
    "joint_kls = []\n",
    "theta_func_losses = []\n",
    "times = []\n",
    "filter_means = []\n",
    "filter_stds = []\n",
    "\n",
    "pbar = tqdm.tqdm(range(0, cfg.data.num_data))\n",
    "\n",
    "for T in pbar:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- Advance timesteps --------------\n",
    "\n",
    "    phi_model.advance_timestep(y[T, :])\n",
    "    theta_grad.advance_timestep()\n",
    "\n",
    "\n",
    "    # ----------- Phi optimization ----------------\n",
    "\n",
    "\n",
    "\n",
    "    if cfg.phi_training.func_type == 'analytic' and T>0:\n",
    "        tmpG = torch.diag(phi_model.G_fn.weight.data.clone())\n",
    "        tmpF = torch.diag(phi_model.F_fn.weight.data.clone())\n",
    "        analytic_kalman_phi_update(phi_model, T, tmpG, tmpF, U, V, y[T,:])\n",
    "\n",
    "    elif cfg.phi_training.func_type == 'Vx_t':\n",
    "        phi_optim = torch.optim.Adam(phi_model.get_phi_T_params(),\n",
    "            lr=cfg.phi_training.phi_lr)\n",
    "        phi_decay = torch.optim.lr_scheduler.StepLR(\n",
    "            phi_optim, 1, cfg.phi_training.phi_lr_decay_gamma\n",
    "        )\n",
    "        for i in range(cfg.phi_training.phi_iters):\n",
    "            phi_optim.zero_grad()\n",
    "            phi_model.populate_phi_grads(y,\n",
    "                cfg.phi_training.phi_minibatch_size)\n",
    "            phi_optim.step()\n",
    "            phi_decay.step()\n",
    "\n",
    "        if T >= cfg.phi_training.window_size - 1:\n",
    "            phi_model.update_V_t(y, cfg.phi_training.V_batch_size)\n",
    "            Vx_optim = torch.optim.Adam(phi_model.get_V_t_params(),\n",
    "                lr=cfg.phi_training.V_lr)\n",
    "            for k in range(cfg.phi_training.V_iters):\n",
    "                Vx_optim.zero_grad()\n",
    "                V_loss, _, _ = phi_model.V_t_loss(y,\n",
    "                    cfg.phi_training.V_minibatch_size)\n",
    "                V_loss.backward()\n",
    "                Vx_optim.step()\n",
    "\n",
    "\n",
    "    # -------------- Theta func training ----------------\n",
    "\n",
    "\n",
    "    if T >= cfg.theta_training.window_size:\n",
    "        if cfg.theta_training.func_type == 'neural_net':\n",
    "            theta_func_optim = torch.optim.Adam(\n",
    "                theta_grad.get_theta_func_TmL_parameters(),\n",
    "                lr=cfg.theta_training.net_lr)\n",
    "            net_inputs, net_targets = theta_grad.generate_training_dataset(\n",
    "                cfg.theta_training.net_dataset_size, y\n",
    "            )\n",
    "            net_inputs = net_inputs.detach()\n",
    "            net_targets = net_targets.detach()\n",
    "\n",
    "            theta_grad.theta_func_TmL.update_normalization(\n",
    "                net_inputs, net_targets, cfg.theta_training.net_norm_decay\n",
    "            )\n",
    "            for i in range(cfg.theta_training.net_iters):\n",
    "                idx = np.random.choice(np.arange(net_inputs.shape[0]),\n",
    "                    (cfg.theta_training.net_minibatch_size,), replace=False)\n",
    "                theta_func_optim.zero_grad()\n",
    "                preds = theta_grad.theta_func_TmL(net_inputs[idx,:])\n",
    "                loss = torch.mean(\n",
    "                    torch.sum((preds - net_targets[idx, :])**2, dim=1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                theta_func_optim.step()\n",
    "                theta_func_losses.append(loss.item())\n",
    "        elif cfg.theta_training.func_type == 'kernel':\n",
    "            kernel_inputs, kernel_targets = theta_grad.generate_training_dataset(\n",
    "                cfg.theta_training.kernel_batch_size, y\n",
    "            )\n",
    "            kernel_inputs = kernel_inputs.detach()\n",
    "            kernel_targets = kernel_targets.detach()\n",
    "            if T == cfg.theta_training.window_size and \\\n",
    "                cfg.theta_training.KRR_init_sigma_median:\n",
    "                theta_grad.theta_func_TmL.krr.kernel.log_sigma.data = \\\n",
    "                    torch.tensor(\n",
    "                        np.log(utils.estimate_median_distance(kernel_inputs)\\\n",
    "                            .astype(float))\n",
    "                    ).to(device)\n",
    "                print(\"Update bandwidth to \", theta_grad.theta_func_TmL.krr.kernel.log_sigma.exp().item())\n",
    "            theta_grad.theta_func_TmL.fit(kernel_inputs, kernel_targets)\n",
    "\n",
    "            kernel_optim = torch.optim.Adam(\n",
    "                theta_grad.theta_func_TmL.parameters(),\n",
    "                lr=cfg.theta_training.train_kernel_lr\n",
    "            )\n",
    "            # Generate new data to train hyperparams on\n",
    "            if cfg.theta_training.KRR_train_sigma or cfg.theta_training.KRR_train_lam:\n",
    "                kernel_inputs, kernel_targets = theta_grad.generate_training_dataset(\n",
    "                    cfg.theta_training.train_kernel_dataset_size, y\n",
    "                )\n",
    "                kernel_inputs = kernel_inputs.detach()\n",
    "                kernel_targets = kernel_targets.detach()\n",
    "                for i in range(cfg.theta_training.train_kernel_iters):\n",
    "                    idx = np.random.choice(np.arange(kernel_inputs.shape[0]),\n",
    "                        (cfg.theta_training.train_kernel_minibatch_size,), replace=False)\n",
    "                    kernel_optim.zero_grad()\n",
    "                    preds = theta_grad.theta_func_TmL(kernel_inputs[idx,:])\n",
    "                    loss = torch.mean(\n",
    "                        torch.sum((preds - kernel_targets[idx,:])**2, dim=1)\n",
    "                    )\n",
    "                    loss.backward()\n",
    "                    kernel_optim.step()\n",
    "        elif cfg.theta_training.func_type == 'analytic_S':\n",
    "            # Compute S_{T-window_size} (T-window_size>0)\n",
    "            if T > cfg.theta_training.window_size:\n",
    "                theta_grad.theta_func_TmL.advance_timestep(\n",
    "                    y[T - cfg.theta_training.window_size],\n",
    "                    phi_model.F_fn.weight.data.clone(),\n",
    "                    phi_model.G_fn.weight.data.clone(),\n",
    "                    qW=phi_model.cond_q_t_mean_net_list[T - cfg.theta_training.window_size].weight.data.clone(),\n",
    "                    qb=phi_model.cond_q_t_mean_net_list[T - cfg.theta_training.window_size].bias.data.clone(),\n",
    "                    qcov_diag=torch.exp(2 * phi_model.cond_q_t_log_std_list[T - cfg.theta_training.window_size])\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- Theta update ----------------\n",
    "\n",
    "\n",
    "    if T > cfg.theta_training.theta_updates_start_T:\n",
    "        theta_optim.zero_grad() \n",
    "        theta_grad.populate_theta_grads(\n",
    "            cfg.theta_training.theta_minibatch_size, y)\n",
    "        theta_optim.step()\n",
    "        Gs.append(G_fn.weight.clone().detach().numpy())\n",
    "        Fs.append(F_fn.weight.clone().detach().numpy())\n",
    "\n",
    "        pbar.set_postfix({\"F MAE\": np.mean(np.abs(Fs[-1] - np.diag(np.array(F)))),\n",
    "                            \"G MAE\": np.mean(np.abs(Gs[-1] - np.diag(np.array(G))))})\n",
    "\n",
    "        rmle.step_size = theta_decay.state_dict()['_last_lr'][0]\n",
    "        rmle.advance_timestep(y[T, :].detach().numpy().copy().reshape((DIM,1)))\n",
    "        rmle_Gs.append(rmle.G.copy())\n",
    "        rmle_Fs.append(rmle.F.copy())\n",
    "\n",
    "        theta_decay.step()\n",
    "\n",
    "\n",
    "    # -------------- Logging --------------------\n",
    "\n",
    "\n",
    "    filter_means.append(phi_model.q_t_mean_list[T].detach().cpu().numpy())\n",
    "    filter_stds.append(phi_model.q_t_log_std_list[T].detach().cpu().numpy())\n",
    "\n",
    "    if T>0:\n",
    "        joint_kls.append(estimate_joint_kl(phi_model, 256,\n",
    "                    kalman_xs_pyt[T, :], kalman_Ps_pyt[T, :, :],\n",
    "                    kalman_xs_pyt[T - 1, :], kalman_Ps_pyt[T - 1, :, :],\n",
    "                    F, U).item())\n",
    "\n",
    "    if (T % (round(max(cfg.data.num_data, cfg.theta_training.num_times_save_data)\\\n",
    "        / cfg.theta_training.num_times_save_data)) == 0) or\\\n",
    "        (T == cfg.data.num_data - 1):\n",
    "\n",
    "        save_np('Gs.npy', np.array(Gs))\n",
    "        save_np('Fs.npy', np.array(Fs))\n",
    "        save_np('rmle_Gs.npy', np.array(rmle_Gs))\n",
    "        save_np('rmle_Fs.npy', np.array(rmle_Fs))\n",
    "        save_np('joint_kls.npy', np.array(joint_kls))\n",
    "        save_np('theta_func_losses.npy', np.array(theta_func_losses))\n",
    "        save_np('times.npy', np.array(times))\n",
    "        save_np('filter_means.npy', np.array(filter_means))\n",
    "        save_np('filter_stds.npy', np.array(filter_stds))\n",
    "        if cfg.save_models:\n",
    "            torch.save(phi_model.state_dict(), saved_models_folder_name + \\\n",
    "                '/phi_model_{}.pt'.format(T))\n",
    "            torch.save(theta_grad.theta_func_TmL.state_dict(),\n",
    "                saved_models_folder_name + '/theta_model_{}.pt'.format(T))\n",
    "            torch.save(theta_optim.state_dict(), saved_models_folder_name +\\\n",
    "                '/theta_optim_{}.pt'.format(T))\n",
    "            torch.save(theta_decay.state_dict(), saved_models_folder_name +\\\n",
    "                '/theta_decay_{}.pt'.format(T))\n",
    "\n",
    "    times.append(time.time()-start_time)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "rmle_F_maes = np.mean(np.abs(np.diagonal(rmle_Fs, axis1=1, axis2=2) - np.diag(F)), 1)\n",
    "F_maes = np.mean(np.abs(Fs - np.diag(F)), 1)\n",
    "rmle_G_maes = np.mean(np.abs(np.diagonal(rmle_Gs, axis1=1, axis2=2) - np.diag(G)), 1)\n",
    "G_maes = np.mean(np.abs(Gs - np.diag(G)), 1)\n",
    "ax1.plot(rmle_F_maes)\n",
    "ax1.plot(F_maes)\n",
    "ax2.plot(rmle_G_maes)\n",
    "ax2.plot(G_maes)\n",
    "plt.show()\n",
    "\n",
    "print(\"F RMLE: \", rmle.F.copy())\n",
    "print(\"G RMLE: \", rmle.G.copy())\n",
    "print(\"F: \", Fs[-1])\n",
    "print(\"G: \", Gs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
