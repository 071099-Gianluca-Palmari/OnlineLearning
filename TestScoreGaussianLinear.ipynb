{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.distributions import MultivariateNormal, Independent, Normal, Categorical\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Utils:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeStore(nn.Module):\n",
    "    \"\"\"\n",
    "        Acts like a ParameterList/ModuleList that just keeps getting longer but\n",
    "        under the hood it only stores the most recent N values.\n",
    "        Does not support removing items from the list.\n",
    "    \"\"\"\n",
    "    def __init__(self, first_val, N, type):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == \"ParameterList\":\n",
    "            self.list = nn.ParameterList([first_val] if first_val else [])\n",
    "        elif type == \"ModuleList\":\n",
    "            self.list = nn.ModuleList([first_val] if first_val else [])\n",
    "        else:\n",
    "            raise ValueError(\"TimeStore unknown type: \" + type)\n",
    "        self.len = 1 if first_val else 0\n",
    "        self.N = N\n",
    "\n",
    "    def append(self, val):\n",
    "        self.list.append(val)\n",
    "        self.len += 1\n",
    "        if len(self.list) > self.N:\n",
    "            self.list = self.list[-self.N:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        adjusted_idx = index - self.len + len(self.list)\n",
    "\n",
    "        if index < 0:\n",
    "            raise ValueError(\"TimeStore does not support negative indexing\")\n",
    "        if index >= self.len:\n",
    "            raise ValueError(\"TimeStore index {} is too large\".format(index))\n",
    "        if adjusted_idx < 0:\n",
    "            raise ValueError(\"TimeStore attempting to access deleted entry\")\n",
    "\n",
    "        return self.list[adjusted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_posterior(y, prior_mean, prior_cov, G, V, G_fn=None):\n",
    "    # Compute posterior mean, cov of N(x; prior_mean, prior_cov) N(y; G @ x, V)\n",
    "    # y: (*, ydim), prior_mean: (*, xdim), prior_cov: (*, xdim, xdim),\n",
    "    # G: (*, ydim, xdim), V: (*, ydim, ydim)\n",
    "    K = prior_cov @ G.transpose(-2, -1) @ torch.linalg.inv(G @ prior_cov @ G.transpose(-2, -1) + V)\n",
    "    I_KG = torch.eye(prior_cov.shape[-1], device=prior_cov.device) - K @ G\n",
    "    post_cov = I_KG @ prior_cov\n",
    "    if G_fn is None:\n",
    "        post_mean = (prior_mean.unsqueeze(-2) @ I_KG.transpose(-2, -1) +\n",
    "                     y.unsqueeze(-2) @ K.transpose(-2, -1)).squeeze(-2)\n",
    "    else:\n",
    "        post_mean = prior_mean + ((y - G_fn(prior_mean)).unsqueeze(-2) @ K.transpose(-2, -1)).squeeze(-2)\n",
    "    return post_mean, post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov(x, y=None, w=None):\n",
    "    assert len(x.shape) == 2\n",
    "    if w is None:\n",
    "        w = 1 / x.shape[0] * torch.ones((x.shape[0], 1)).to(x.device)\n",
    "    else:\n",
    "        w = w.view(x.shape[0], 1)\n",
    "    x_centred = x - (x * w).sum(0)\n",
    "    if y is None:\n",
    "        y_centred = x_centred\n",
    "    else:\n",
    "        y_centred = y - (y * w).sum(0)\n",
    "    cov = (w * x_centred).t() @ y_centred / (1 - (w**2).sum())\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Data\\text{ }Generation:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = cfg.data.dim\n",
    "\n",
    "if not cfg.data.diagFG:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if cfg.data.path_to_data is None:\n",
    "    F, G, U, V = construct_HMM_matrices(dim=DIM,\n",
    "                                        F_eigvals=np.random.uniform(\n",
    "                                            cfg.data.F_min_eigval,\n",
    "                                            cfg.data.F_max_eigval, (DIM)),\n",
    "                                        G_eigvals=np.random.uniform(\n",
    "                                            cfg.data.G_min_eigval,\n",
    "                                            cfg.data.G_max_eigval, (DIM)),\n",
    "                                        U_std=cfg.data.U_std,\n",
    "                                        V_std=cfg.data.V_std,\n",
    "                                        diag=cfg.data.diagFG)\n",
    "\n",
    "    data_gen = GaussianHMM(xdim=DIM, ydim=DIM, F=F, G=G, U=U, V=V)\n",
    "    x_np, y_np = data_gen.generate_data(cfg.data.num_data)\n",
    "\n",
    "    save_np('datapoints.npy', np.stack((x_np, y_np)))\n",
    "    save_np('F.npy', F)\n",
    "    save_np('G.npy', G)\n",
    "    save_np('U.npy', U)\n",
    "    save_np('V.npy', V)\n",
    "else:\n",
    "    path_to_data = hydra.utils.to_absolute_path(cfg.data.path_to_data) + '/'\n",
    "    F, G, U, V = np.load(path_to_data + 'F.npy'), \\\n",
    "                np.load(path_to_data + 'G.npy'), \\\n",
    "                np.load(path_to_data + 'U.npy'), \\\n",
    "                np.load(path_to_data + 'V.npy')\n",
    "    xystack = np.load(path_to_data + 'datapoints.npy')\n",
    "    x_np = xystack[0, :, :]\n",
    "    y_np = xystack[1, :, :]\n",
    "\n",
    "print(\"True F: \", F)\n",
    "print(\"True G: \", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Kalman-Filter:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_kalman_phi_update(model, T, G, F, U, V, y_T):\n",
    "    '''\n",
    "    xtp1 = Fxtp + w, w sim N(0,U)\n",
    "    yt = Gxt + v, w sim N(0,V)\n",
    "\n",
    "    Pp: cov of the nonadapted error\n",
    "    K: Kalman gain\n",
    "    z: the innovation\n",
    "    new_mean: adapted x\n",
    "    new_cov: new error covariance\n",
    "    cond_cov: \n",
    "    '''\n",
    "    \n",
    "    prev_mean = model.q_t_mean_list[T-1].reshape(model.xdim, 1)\n",
    "    prev_cov = torch.diag(torch.exp(2*model.q_t_log_std_list[T-1]))\n",
    "    y_T = y_T.reshape(model.ydim, 1)\n",
    "\n",
    "    xp = F @ prev_mean\n",
    "    Pp = F @ prev_cov @ F.T + U\n",
    "    S = G @ Pp @ G.T + V\n",
    "    K = Pp @ G.T @ torch.inverse(S)\n",
    "    z = y_T - G @ xp\n",
    "\n",
    "    new_mean = xp + K @ z\n",
    "    new_cov = (torch.eye(model.xdim).to(device) - K @ G) @ Pp\n",
    "\n",
    "    cond_cov = torch.inverse(torch.inverse(prev_cov) + \\\n",
    "        F.T @ torch.inverse(U) @ F) \n",
    "    cond_weight = cond_cov @ F.T @ torch.inverse(U)\n",
    "    cond_bias = cond_cov @ torch.inverse(prev_cov) @ prev_mean\n",
    "\n",
    "    model.q_t_mean_list[T].data = new_mean.reshape(model.xdim)\n",
    "    model.q_t_log_std_list[T].data = 0.5 * torch.log(torch.diag(new_cov))\n",
    "    model.cond_q_t_mean_net_list[T].weight.data = cond_weight\n",
    "    model.cond_q_t_mean_net_list[T].bias.data = cond_bias.reshape(model.xdim)\n",
    "    model.cond_q_t_log_std_list[T].data = 0.5 * torch.log(torch.diag(cond_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Model\\text{ }parametrization:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the observalbe's transiton map\n",
    "class F_Module(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.register_parameter('weight',\n",
    "                nn.Parameter(torch.zeros(DIM)))\n",
    "            self.F_mean_fn = lambda x, t: self.weight * x\n",
    "            self.F_cov_fn = lambda x, t: U\n",
    "            self.F_cov = U\n",
    "\n",
    "        def forward(self, x, t=None):\n",
    "            return Independent(Normal(self.F_mean_fn(x, t),\n",
    "                torch.sqrt(torch.diag(U))), 1)\n",
    "\n",
    "# we define the latent's transiton map\n",
    "class G_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_parameter('weight',\n",
    "            nn.Parameter(torch.zeros(DIM)))\n",
    "        self.G_mean_fn = lambda x, t: self.weight * x\n",
    "        self.G_cov = V\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        return Independent(Normal(self.G_mean_fn(x, t),\n",
    "            torch.sqrt(torch.diag(V))), 1)\n",
    "            \n",
    "# we define the initialization class\n",
    "class p_0_dist_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mean_0 = mean_0\n",
    "        self.cov_0 = torch.eye(DIM, device=device) * std_0 ** 2\n",
    "\n",
    "    def forward(self):\n",
    "        return Independent(Normal(mean_0, std_0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the two linking modules and the initial prior on the latent variable.\n",
    "F_fn = F_Module().to(device)\n",
    "G_fn = G_Module().to(device)\n",
    "p_0_dist = p_0_dist_module().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Parameters:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of parameters to be learned\n",
    "G_theta_dim = sum([p.numel() for p in G_fn.parameters()])\n",
    "F_theta_dim = sum([p.numel() for p in F_fn.parameters()])\n",
    "print(\"G theta dim\", G_theta_dim)\n",
    "print(\"F theta dim\", F_theta_dim)\n",
    "if cfg.theta_training.matrices_to_learn == 'F':\n",
    "    theta_dim = F_theta_dim\n",
    "elif cfg.theta_training.matrices_to_learn == 'G':\n",
    "    theta_dim = G_theta_dim\n",
    "elif cfg.theta_training.matrices_to_learn == 'FG':\n",
    "    theta_dim = F_theta_dim + G_theta_dim\n",
    "else:\n",
    "    raise ValueError(cfg.theta_training.matrices_to_learn)\n",
    "print(\"Theta dim\", theta_dim)\n",
    "\n",
    "# get the model parameters\n",
    "def get_model_parameters():\n",
    "        if cfg.theta_training.matrices_to_learn == 'F':\n",
    "            return F_fn.parameters()\n",
    "        elif cfg.theta_training.matrices_to_learn == 'G':\n",
    "            return G_fn.parameters()\n",
    "        elif cfg.theta_training.matrices_to_learn == 'FG':\n",
    "            return [*F_fn.parameters(), *G_fn.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Variational\\text{ }Model\\text{ }Specifications-(\\phi):}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_q_mean_net_constructor():\n",
    "        return torch.nn.Linear(DIM, DIM).to(device)\n",
    "\n",
    "if cfg.phi_training.func_type == 'Vx_t':\n",
    "    print(\"Using phi model Vx_t\")\n",
    "\n",
    "    sigma = cfg.phi_training.KRR_sigma\n",
    "    lam = cfg.phi_training.KRR_lambda\n",
    "    train_sigma = cfg.phi_training.KRR_train_sigma\n",
    "    train_lam = cfg.phi_training.KRR_train_lam\n",
    "\n",
    "    def KRR_constructor():\n",
    "        return models.KernelRidgeRegressor(models.MaternKernel(\n",
    "            sigma=sigma, lam=lam, train_sigma=train_sigma, train_lam=train_lam)).to(device)\n",
    "\n",
    "    phi_model = models.Vx_t_phi_t_Model(\n",
    "        device, DIM, DIM, torch.randn(DIM, device=device),\n",
    "        torch.zeros(DIM, device=device), cond_q_mean_net_constructor,\n",
    "        torch.zeros(DIM, device=device), F_fn, G_fn, p_0_dist,\n",
    "        cfg.phi_training.phi_t_init_method,\n",
    "        cfg.phi_training.window_size,\n",
    "        KRR_constructor, cfg.phi_training.KRR_init_sigma_median,\n",
    "        cfg.phi_training.approx_decay,\n",
    "        cfg.phi_training.approx_with_filter,\n",
    "        max(cfg.phi_training.window_size, cfg.theta_training.window_size)+1\n",
    "    )\n",
    "\n",
    "elif cfg.phi_training.func_type == 'analytic':\n",
    "    print(\"Using analytic phi updates\")\n",
    "    phi_model = models.NonAmortizedModelBase(\n",
    "        device, DIM, DIM, torch.zeros(DIM, device=device),\n",
    "        torch.zeros(DIM, device=device), cond_q_mean_net_constructor,\n",
    "        torch.zeros(DIM, device=device), F_fn, G_fn, p_0_dist,\n",
    "        'last', 1, cfg.theta_training.window_size + 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Real\\text{ }Model\\text{ }Specifications-(\\theta)}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{Training\\text{ }Specifications}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_optim = torch.optim.Adam(get_model_parameters(),\n",
    "        lr=cfg.theta_training.theta_lr)\n",
    "if cfg.theta_training.theta_lr_decay_type == 'exponential':\n",
    "    theta_decay = torch.optim.lr_scheduler.StepLR(theta_optim,\n",
    "        step_size=1, gamma=np.exp(\n",
    "            (1/cfg.theta_training.num_steps_theta_lr_oom_drop) * np.log(0.1)))\n",
    "elif cfg.theta_training.theta_lr_decay_type == 'robbins-monro':\n",
    "    lr_decay_rate = cfg.theta_training.robbins_monro_theta_lr_decay_rate\n",
    "    lr_decay_bias = cfg.theta_training.robbins_monro_theta_lr_decay_bias\n",
    "    theta_decay = torch.optim.lr_scheduler.LambdaLR(theta_optim,\n",
    "        lr_lambda=lambda epoch: lr_decay_bias / (lr_decay_bias + epoch ** lr_decay_rate))\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underline{Training:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Gs = []\n",
    "Fs = []\n",
    "rmle_Gs = []\n",
    "rmle_Fs = []\n",
    "joint_kls = []\n",
    "theta_func_losses = []\n",
    "times = []\n",
    "filter_means = []\n",
    "filter_stds = []\n",
    "\n",
    "pbar = tqdm(range(0, cfg.data.num_data))\n",
    "\n",
    "for T in pbar:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- Advance timesteps --------------\n",
    "\n",
    "    phi_model.advance_timestep(y[T, :])\n",
    "    theta_grad.advance_timestep()\n",
    "\n",
    "\n",
    "    # ----------- Phi optimization ----------------\n",
    "\n",
    "\n",
    "\n",
    "    if cfg.phi_training.func_type == 'analytic' and T>0:\n",
    "        tmpG = torch.diag(phi_model.G_fn.weight.data.clone())\n",
    "        tmpF = torch.diag(phi_model.F_fn.weight.data.clone())\n",
    "        analytic_kalman_phi_update(phi_model, T, tmpG, tmpF, U, V, y[T,:])\n",
    "\n",
    "    elif cfg.phi_training.func_type == 'Vx_t':\n",
    "        phi_optim = torch.optim.Adam(phi_model.get_phi_T_params(),\n",
    "            lr=cfg.phi_training.phi_lr)\n",
    "        phi_decay = torch.optim.lr_scheduler.StepLR(\n",
    "            phi_optim, 1, cfg.phi_training.phi_lr_decay_gamma\n",
    "        )\n",
    "        for i in range(cfg.phi_training.phi_iters):\n",
    "            phi_optim.zero_grad()\n",
    "            phi_model.populate_phi_grads(y,\n",
    "                cfg.phi_training.phi_minibatch_size)\n",
    "            phi_optim.step()\n",
    "            phi_decay.step()\n",
    "\n",
    "        if T >= cfg.phi_training.window_size - 1:\n",
    "            phi_model.update_V_t(y, cfg.phi_training.V_batch_size)\n",
    "            Vx_optim = torch.optim.Adam(phi_model.get_V_t_params(),\n",
    "                lr=cfg.phi_training.V_lr)\n",
    "            for k in range(cfg.phi_training.V_iters):\n",
    "                Vx_optim.zero_grad()\n",
    "                V_loss, _, _ = phi_model.V_t_loss(y,\n",
    "                    cfg.phi_training.V_minibatch_size)\n",
    "                V_loss.backward()\n",
    "                Vx_optim.step()\n",
    "\n",
    "\n",
    "    # -------------- Theta func training ----------------\n",
    "\n",
    "\n",
    "    if T >= cfg.theta_training.window_size:\n",
    "        if cfg.theta_training.func_type == 'neural_net':\n",
    "            theta_func_optim = torch.optim.Adam(\n",
    "                theta_grad.get_theta_func_TmL_parameters(),\n",
    "                lr=cfg.theta_training.net_lr)\n",
    "            net_inputs, net_targets = theta_grad.generate_training_dataset(\n",
    "                cfg.theta_training.net_dataset_size, y\n",
    "            )\n",
    "            net_inputs = net_inputs.detach()\n",
    "            net_targets = net_targets.detach()\n",
    "\n",
    "            theta_grad.theta_func_TmL.update_normalization(\n",
    "                net_inputs, net_targets, cfg.theta_training.net_norm_decay\n",
    "            )\n",
    "            for i in range(cfg.theta_training.net_iters):\n",
    "                idx = np.random.choice(np.arange(net_inputs.shape[0]),\n",
    "                    (cfg.theta_training.net_minibatch_size,), replace=False)\n",
    "                theta_func_optim.zero_grad()\n",
    "                preds = theta_grad.theta_func_TmL(net_inputs[idx,:])\n",
    "                loss = torch.mean(\n",
    "                    torch.sum((preds - net_targets[idx, :])**2, dim=1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                theta_func_optim.step()\n",
    "                theta_func_losses.append(loss.item())\n",
    "        elif cfg.theta_training.func_type == 'kernel':\n",
    "            kernel_inputs, kernel_targets = theta_grad.generate_training_dataset(\n",
    "                cfg.theta_training.kernel_batch_size, y\n",
    "            )\n",
    "            kernel_inputs = kernel_inputs.detach()\n",
    "            kernel_targets = kernel_targets.detach()\n",
    "            if T == cfg.theta_training.window_size and \\\n",
    "                cfg.theta_training.KRR_init_sigma_median:\n",
    "                theta_grad.theta_func_TmL.krr.kernel.log_sigma.data = \\\n",
    "                    torch.tensor(\n",
    "                        np.log(utils.estimate_median_distance(kernel_inputs)\\\n",
    "                            .astype(float))\n",
    "                    ).to(device)\n",
    "                print(\"Update bandwidth to \", theta_grad.theta_func_TmL.krr.kernel.log_sigma.exp().item())\n",
    "            theta_grad.theta_func_TmL.fit(kernel_inputs, kernel_targets)\n",
    "\n",
    "            kernel_optim = torch.optim.Adam(\n",
    "                theta_grad.theta_func_TmL.parameters(),\n",
    "                lr=cfg.theta_training.train_kernel_lr\n",
    "            )\n",
    "            # Generate new data to train hyperparams on\n",
    "            if cfg.theta_training.KRR_train_sigma or cfg.theta_training.KRR_train_lam:\n",
    "                kernel_inputs, kernel_targets = theta_grad.generate_training_dataset(\n",
    "                    cfg.theta_training.train_kernel_dataset_size, y\n",
    "                )\n",
    "                kernel_inputs = kernel_inputs.detach()\n",
    "                kernel_targets = kernel_targets.detach()\n",
    "                for i in range(cfg.theta_training.train_kernel_iters):\n",
    "                    idx = np.random.choice(np.arange(kernel_inputs.shape[0]),\n",
    "                        (cfg.theta_training.train_kernel_minibatch_size,), replace=False)\n",
    "                    kernel_optim.zero_grad()\n",
    "                    preds = theta_grad.theta_func_TmL(kernel_inputs[idx,:])\n",
    "                    loss = torch.mean(\n",
    "                        torch.sum((preds - kernel_targets[idx,:])**2, dim=1)\n",
    "                    )\n",
    "                    loss.backward()\n",
    "                    kernel_optim.step()\n",
    "        elif cfg.theta_training.func_type == 'analytic_S':\n",
    "            # Compute S_{T-window_size} (T-window_size>0)\n",
    "            if T > cfg.theta_training.window_size:\n",
    "                theta_grad.theta_func_TmL.advance_timestep(\n",
    "                    y[T - cfg.theta_training.window_size],\n",
    "                    phi_model.F_fn.weight.data.clone(),\n",
    "                    phi_model.G_fn.weight.data.clone(),\n",
    "                    qW=phi_model.cond_q_t_mean_net_list[T - cfg.theta_training.window_size].weight.data.clone(),\n",
    "                    qb=phi_model.cond_q_t_mean_net_list[T - cfg.theta_training.window_size].bias.data.clone(),\n",
    "                    qcov_diag=torch.exp(2 * phi_model.cond_q_t_log_std_list[T - cfg.theta_training.window_size])\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- Theta update ----------------\n",
    "\n",
    "\n",
    "    if T > cfg.theta_training.theta_updates_start_T:\n",
    "        theta_optim.zero_grad() \n",
    "        theta_grad.populate_theta_grads(\n",
    "            cfg.theta_training.theta_minibatch_size, y)\n",
    "        theta_optim.step()\n",
    "        Gs.append(G_fn.weight.clone().detach().numpy())\n",
    "        Fs.append(F_fn.weight.clone().detach().numpy())\n",
    "\n",
    "        pbar.set_postfix({\"F MAE\": np.mean(np.abs(Fs[-1] - np.diag(np.array(F)))),\n",
    "                            \"G MAE\": np.mean(np.abs(Gs[-1] - np.diag(np.array(G))))})\n",
    "\n",
    "        rmle.step_size = theta_decay.state_dict()['_last_lr'][0]\n",
    "        rmle.advance_timestep(y[T, :].detach().numpy().copy().reshape((DIM,1)))\n",
    "        rmle_Gs.append(rmle.G.copy())\n",
    "        rmle_Fs.append(rmle.F.copy())\n",
    "\n",
    "        theta_decay.step()\n",
    "\n",
    "\n",
    "    # -------------- Logging --------------------\n",
    "\n",
    "\n",
    "    filter_means.append(phi_model.q_t_mean_list[T].detach().cpu().numpy())\n",
    "    filter_stds.append(phi_model.q_t_log_std_list[T].detach().cpu().numpy())\n",
    "\n",
    "    if T>0:\n",
    "        joint_kls.append(estimate_joint_kl(phi_model, 256,\n",
    "                    kalman_xs_pyt[T, :], kalman_Ps_pyt[T, :, :],\n",
    "                    kalman_xs_pyt[T - 1, :], kalman_Ps_pyt[T - 1, :, :],\n",
    "                    F, U).item())\n",
    "\n",
    "    if (T % (round(max(cfg.data.num_data, cfg.theta_training.num_times_save_data)\\\n",
    "        / cfg.theta_training.num_times_save_data)) == 0) or\\\n",
    "        (T == cfg.data.num_data - 1):\n",
    "\n",
    "        save_np('Gs.npy', np.array(Gs))\n",
    "        save_np('Fs.npy', np.array(Fs))\n",
    "        save_np('rmle_Gs.npy', np.array(rmle_Gs))\n",
    "        save_np('rmle_Fs.npy', np.array(rmle_Fs))\n",
    "        save_np('joint_kls.npy', np.array(joint_kls))\n",
    "        save_np('theta_func_losses.npy', np.array(theta_func_losses))\n",
    "        save_np('times.npy', np.array(times))\n",
    "        save_np('filter_means.npy', np.array(filter_means))\n",
    "        save_np('filter_stds.npy', np.array(filter_stds))\n",
    "        if cfg.save_models:\n",
    "            torch.save(phi_model.state_dict(), saved_models_folder_name + \\\n",
    "                '/phi_model_{}.pt'.format(T))\n",
    "            torch.save(theta_grad.theta_func_TmL.state_dict(),\n",
    "                saved_models_folder_name + '/theta_model_{}.pt'.format(T))\n",
    "            torch.save(theta_optim.state_dict(), saved_models_folder_name +\\\n",
    "                '/theta_optim_{}.pt'.format(T))\n",
    "            torch.save(theta_decay.state_dict(), saved_models_folder_name +\\\n",
    "                '/theta_decay_{}.pt'.format(T))\n",
    "\n",
    "    times.append(time.time()-start_time)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "rmle_F_maes = np.mean(np.abs(np.diagonal(rmle_Fs, axis1=1, axis2=2) - np.diag(F)), 1)\n",
    "F_maes = np.mean(np.abs(Fs - np.diag(F)), 1)\n",
    "rmle_G_maes = np.mean(np.abs(np.diagonal(rmle_Gs, axis1=1, axis2=2) - np.diag(G)), 1)\n",
    "G_maes = np.mean(np.abs(Gs - np.diag(G)), 1)\n",
    "ax1.plot(rmle_F_maes)\n",
    "ax1.plot(F_maes)\n",
    "ax2.plot(rmle_G_maes)\n",
    "ax2.plot(G_maes)\n",
    "plt.show()\n",
    "\n",
    "print(\"F RMLE: \", rmle.F.copy())\n",
    "print(\"G RMLE: \", rmle.G.copy())\n",
    "print(\"F: \", Fs[-1])\n",
    "print(\"G: \", Gs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
